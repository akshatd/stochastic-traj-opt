\documentclass{article}
\usepackage[a4paper, total={7in, 8in}]{geometry}
\usepackage{graphicx} % Required for inserting images
\usepackage{amsmath} % Required for some math elements
\usepackage{hyperref} % Required for hyperlinks
\usepackage{amssymb} % Required for probability symbols

\title{MultifidelityTrajectoryOptimization}
\author{goroda }
\date{October 2024}

\begin{document}

\maketitle

\begin{abstract}
  TODO: abstract
\end{abstract}

\section{Introduction}
TODO: introduction
\section{Background}
TODO: background

\section{Problem Formulation}
This secition describes how the state space model of the dynamics is set up, along with the controller design and cost function for the trajectory optimization problem.
\subsection{Mass-Spring-Damper System}
For this paper, a simplified trajectory optimization problem is considered that is easy to understand and analyze.
The objective is to solve a reference tracking problem for the position and velocity of a simple mass-spring-damper system.
The mass-spring-damper system has the following parameters:
\begin{itemize}
  \item Mass: $m = 5$ g
  \item Spring constant: $k = 2$ N/m
  \item Damping coefficient: $c = 0.5$ Ns/m
    % \item Initial position: $x_0 = 0$ m
    % \item Initial velocity: $\dot{x}_0 = 0$ m/s
\end{itemize}

Given an external force $F$ and the acceleration of the system $\ddot{x}$,
the dynamics of the system can be written as the equality of forces acting on the mass:

\begin{equation}
  m\ddot{x} = F - c\dot{x} - kx
  \implies \ddot{x} = \frac{F}{m} - \frac{c}{m}\dot{x} - \frac{k}{m}x
\end{equation}

The position $x$ and velocity $\dot{x}$ of the mass-spring-damper system can be changed using the control input $u$,
which applies the force $F$ on the mass. This yields the following dynamics

\begin{equation}
  \begin{bmatrix}
    \dot{x} \\
    \ddot{x}
  \end{bmatrix} =
  \begin{bmatrix}
    0 & 1 \\
    -\frac{k}{m}  -\frac{c}{m}
  \end{bmatrix}
  \begin{bmatrix}
    x \\
    \dot{x}
  \end{bmatrix} +
  \begin{bmatrix}
    0 \\
    \frac{1}{m}
  \end{bmatrix} u
\end{equation}

For a closed-loop system, the output $y$ of the system needs to be observed, which is both the position and velocity in this case.

\begin{equation}
  y =
  \begin{bmatrix}
    1 & 0 \\
    0 & 1
  \end{bmatrix}
  \begin{bmatrix}
    x \\
    \dot{x}
  \end{bmatrix}
\end{equation}

This yields an LTI system in the form of $\dot{x} = Ax + Bu$ and $y = Cx$, where

\begin{equation}
  \begin{aligned}
    A &=
    \begin{bmatrix}
      0 & 1 \\
      -\frac{k}{m} & -\frac{c}{m}
    \end{bmatrix} \\
    B &=
    \begin{bmatrix}
      0 \\
      \frac{1}{m}
    \end{bmatrix} \\
    C &=
    \begin{bmatrix}
      1 & 0 \\
      0 & 1
    \end{bmatrix}
  \end{aligned}
\end{equation}

\subsection{LQR Controller} \label{lqr_ctrl}
We set up an LQR controller to track the reference position and velocity of the mass-spring-damper system.
An LQR controller was chosen because it has a quadratic cost function that can be minimized analytically and gives us a
good reference for the trajectory optimization problem.

To set up the LQR controller, we need to define the cost function of the system, which consists of:

\begin{itemize}
  \item $u_0, u_1 ... u_{N-1}$ : control inputs at each timestep of the trajectory
  \item $J$ : total cost to be minimized
  \item $N$ : time horizon
  \item $x_k$ : state vector at time step $k$
  \item $u_k$ : control input vector at time step $k$
  \item $\Delta u_k = u_k - u_{k-1}$ : change in control input at time step $k$
  \item $r_k$ : reference output at time step $k$
  \item $e_k = y_k - r_k = Cx - r_k$ : error in the output at time step $k$
  \item $Q_e$ : error cost matrix
  \item $R$ : control cost matrix
\end{itemize}
The LQR controller minimizes the cost function

\begin{equation}
  \min_{u_0, u_1 ... u_{N-1}}J = \sum_{k=0}^{N-1} (e_k^T Q_e e_k + \Delta u_k^T R \Delta u_k)
\end{equation}

\subsection{Stochastic LQR Controller}
We will consider the case where the initial state $x_0$ is stochastic, with mean $\mu$ and covariance $\Sigma$.
The control input vector $U$ is still deterministic.

In this case, the state vector $X$ is stochastic, owing to the the initial condition $x_0$ being a vector of random variables. The state propagation equations are now expectations

\begin{equation}
  \begin{aligned}
    \mathbb{E}[x_1] &= \mathbb{E}[A x_0 + B u_0] = A \mathbb{E}[x_0] + B u_0 \\
    \mathbb{E}[x_2] &= \mathbb{E}[A x_1 + B u_1] = A \mathbb{E}[x_1] + B u_1 = A^2 \mathbb{E}[x_0] + A B u_0 + B u_1 \\
    \vdots \\
    \mathbb{E}[x_N] &= \mathbb{E}[A x_{N-1} + B u_{N-1}] = A \mathbb{E}[x_{N-1}] + B u_{N-1} \\
    &= A^N \mathbb{E}[x_0] + A^{N-1} B u_0 + A^{N-2} B u_1 + \ldots + B u_{N-1} \\
  \end{aligned}
\end{equation}

The matrices $S$ and $M$ remain the same, and we have the following expression for the expected value of the state vector $X$

$$
\mathbb{E}[X] = S U + M \mathbb{E}[x_0]
$$

The cost matrices $\bar{Q}$ and $\bar{R}$ are also the same. The cost function $J$ is now a random variable,
with an expected value $\mathbb{E}[J]$ and variance $Var[J]$.

The expected value of the cost function over all possible realizations of the initial conditions is
$$
\begin{aligned}
  \mathbb{E}[J] &= \mathbb{E}[x_N^T P x_N + \sum_{k=0}^{N-1} (x_k^T Q x_k + u_k^T R u_k)] \\
  &= \mathbb{E}[X^T \bar{Q} X + U^T \bar{R} U + x_0^T Q x_0] \\
  &= \mathbb{E}[X^T \bar{Q} X] + \mathbb{E}[U^T \bar{R} U] + \mathbb{E}[x_0^T Q x_0] \\
  &= \mathbb{E}[(S U + M x_0)^T \bar{Q} (S U + M x_0)] + U^T \bar{R} U + \mathbb{E}[x_0^T Q x_0] \\
  &= \mathbb{E}[U^T S^T \bar{Q} S U + U^T S^T \bar{Q} M x_0 + x_0^T M^T \bar{Q} S U + x_0^T M^T \bar{Q} M x_0] + U^T \bar{R} U + \mathbb{E}[x_0^T] Q \mathbb{E}[x_0] + \text{tr}(Q \text{Cov}[x_0]) \\
  &= U^T S^T \bar{Q} S U + \mathbb{E}[U^T S^T \bar{Q} M x_0] + \mathbb{E}[x_0^T M^T \bar{Q} S U] + \mathbb{E}[x_0^T M^T \bar{Q} M x_0] + U^T \bar{R} U + \mathbb{E}[x_0^T] Q \mathbb{E}[x_0] + \text{tr}(Q \text{Cov}[x_0]) \\
  &= U^T (S^T \bar{Q} S + \bar{R}) U + U^T S^T \bar{Q} M \mathbb{E}[x_0] + \mathbb{E}[x_0^T] M^T \bar{Q} S U + \mathbb{E}[x_0^T M^T \bar{Q} M x_0] + \mathbb{E}[x_0^T] Q \mathbb{E}[x_0] + \text{tr}(Q \text{Cov}[x_0]) \\
  &= U^T (S^T \bar{Q} S + \bar{R}) U + \mathbb{E}[x_0^T] M^T \bar{Q} S U + \mathbb{E}[x_0^T] M^T \bar{Q} S U + \mathbb{E}[x_0^T] M^T \bar{Q} M \mathbb{E}[x_0] + \text{tr}(M^T \bar{Q} M \text{Cov}[x_0]) \hdots \\
  & \quad + \mathbb{E}[x_0^T] Q \mathbb{E}[x_0] + \text{tr}(Q \text{Cov}[x_0]) \\
  &= U^T (S^T \bar{Q} S + \bar{R}) U + 2 \mathbb{E}[x_0^T] M^T \bar{Q} S U + \mathbb{E}[x_0^T] (M^T \bar{Q} M + Q) \mathbb{E}[x_0] + \text{tr}((M^T \bar{Q} M + Q) \text{Cov}[x_0]) \\
\end{aligned}
$$

Assuming $\mathbb{E}[x_0]$ and $\text{Cov}[x_0]$ is calculated using a Monte Carlo estimator, we can write the cost function as

$$
\begin{aligned}
  H &= S^T \bar{Q} S + \bar{R} = H^T \\
  q &= (\mathbb{E}[x_0^T] M^T \bar{Q} S)^T = S^T \bar{Q} M \mathbb{E}[x_0] \\
  c &= \mathbb{E}[x_0^T] (M^T \bar{Q} M + Q) \mathbb{E}[x_0] + \text{tr}((M^T \bar{Q} M + Q) \text{Cov}[x_0])
\end{aligned}
$$

The cost function can now be written as a quadratic function of the control inputs $U$

$$
J = U^T H U + 2 q^T U + c
$$

Which has a gradient with respect to $U$ as

$$
\Delta J = 2 H U + 2 q^T
$$

Assuming minimum exists, we can set the gradient to zero to get the optimal control input $U^*$

$$
\begin{aligned}
  \Delta J &= 2 H U + 2 q^T = 0 \\
  \implies U^* &= -H^{-1} q
\end{aligned}
$$

% ref for quadratic form: https://en.wikipedia.org/wiki/Quadratic_form_(statistics)

Tthe cost function can be rewritten as

$$
\begin{aligned}
  J &= U^T(S^T \bar{Q} S + \bar{R}) U + 2 x_0^T M^T \bar{Q} S U + x_0^T(M^T \bar{Q} M + Q) x_0 \\
  \text{Let}& \\
  K &= U^T(S^T \bar{Q} S + \bar{R}) U \\
  L &= 2 M^T \bar{Q} S U \\
  N &= M^T \bar{Q} M + Q = N^T \\
  &\text{(quadratic multiplication by diagnonal($\bar{Q}$) results in a symmetric, $Q$ is symmetric)} \\
  \text{Then}& \\
  J &= K + x_0^T L + x_0^T N x_0 \\
\end{aligned}
$$

Then, the expectation and variance of the cost function is as follows. Take note that the random variable here is $x_0$ and $\mathbb{E}[x_0]$ is a scalar.

% ref:
% - https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf
% - https://math.stackexchange.com/questions/2163694/expectation-of-quartic-form-for-multivariate-gaussian/4247240#4247240
% - https://math.stackexchange.com/questions/1302882/variance-of-a-quadratic-form

\begin{equation}
  \begin{aligned}
    \mathbb{E}[J] &= \mathbb{E}[K] + \mathbb{E}[x_0^T L] + \mathbb{E}[x_0^T N x_0] \\
    &= K + \mathbb{E}[x_0^T] L + \mathbb{E}[x_0^T] N \mathbb{E}[x_0] + \text{tr}(N \text{Cov}[x_0]) \\
    Var[J] &= Var[K + x_0^T L + x_0^T N x_0] \\
    &\text{note: $K$ is deterministic, so its variance is 0} \\
    & \text{note: $x_0^T L$ is a scalar, $x_0^T L = L^T x_0$} \\
    &= Var[L^T x_0 + x_0^T N x_0] \\
    &= 2\text{tr}(N \text{Cov}[x_0] N \text{Cov}[x_0]) + 4 \mathbb{E}[x_0^T]N\text{Cov}[x_0]N\mathbb{E}[x_0] + 4 L^T \text{Cov}[x_0] N \mathbb{E}[x_0] + L^T \text{Cov}[x_0] L \\
  \end{aligned}
\end{equation}

To determine the covariance between the costs of two solutions $U_1$ and $U_2$, the components of $J$ that depend on $U$
need to be written in terms of $U_1$ and $U_2$.

For $U_1$, the cost function is

$$
\begin{aligned}
  J(U_1) &= K_1 + x_0^T L_1 + x_0^T N x_0 \\
  \text{where} \\
  K_1 &= U_1^T(S^T \bar{Q} S + \bar{R}) U_1 \\
  L_1 &= 2 M^T \bar{Q} S U_1 \\
\end{aligned}
$$

Similarly, for $U_2$, the cost function is

$$
\begin{aligned}
  J(U_2) &= K_2 + x_0^T L_2 + x_0^T N x_0 \\
  \text{where} \\
  K_2 &= U_2^T(S^T \bar{Q} S + \bar{R}) U_2 \\
  L_2 &= 2 M^T \bar{Q} S U_2 \\
\end{aligned}
$$

% ref:
% - https://math.stackexchange.com/questions/1302882/variance-of-a-quadratic-form
% - https://stats.stackexchange.com/questions/303466/prove-that-mathrmcovxtax-xtbx-2-mathrmtra-sigma-b-sigma-4-mu
% - https://stats.stackexchange.com/questions/35084/covariance-of-a-linear-and-quadratic-form-of-a-multivariate-normal
% - https://www.researchgate.net/publication/243770190_Quadratic_Forms_in_Random_Variables P74, 3.2d.9
the covariance between their costs can be calculated as
\begin{equation}
  \begin{aligned}
    \text{Cov}[J(U_1), J(U_2)] &= \text{Cov}[K_1 + x_0^T L_1 + x_0^T N x_0, K_2 + x_0^T L_2 + x_0^T N x_0] \\
    &= \text{Cov}[x_0^T L_1 + x_0^T N x_0, x_0^T L_2 + x_0^T N x_0] \text{ (covariance is shift invariant)} \\
    &= \text{Cov}[x_0^T L_1, x_0^T L_2] + \text{Cov}[x_0^T L_1, x_0^T N x_0] + \text{Cov}[x_0^T N x_0, x_0^T L_2] + \text{Cov}[x_0^T N x_0, x_0^T N x_0] \\
    &= \text{Cov}[ L_1^T x_0, L_2^T x_0] + \text{Cov}[L_1^T x_0, x_0^T N x_0] + \text{Cov}[x_0^T N x_0, L_2^T x_0] + \text{Cov}[x_0^T N x_0, x_0^T N x_0] \\
    &= L_1^T \text{Cov}[x_0] L_2 + \text{Cov}[x_0^T N x_0, L_1^T x_0] + \text{Cov}[x_0^T N x_0, x_0^T L_2] + \text{var}[x_0^T N x_0] \\
    &= L_1^T \text{Cov}[x_0] L_2 + 2\mathbb{E}[x_0^T] N \text{Cov}[x_0] L_1 + 2\mathbb{E}[x_0^T] N \text{Cov}[x_0] L_2 \hdots \\
    & \quad 4 \mathbb{E}[x_0^T]N\text{Cov}[x_0]N\mathbb{E}[x_0] + 2\text{tr}((N \text{Cov}[x_0])^2) \\
  \end{aligned}
\end{equation}

\section{Multi Level Monte Carlo}

This section formulates the stochastic trajectory optimization problem as a multi-level Monte Carlo problem.

\subsection{Comparing optimums of high and low fidelity models} \label{comp_opt}

To determine if the low fidelity model can be used as a control variate, we need to determine if it coreelates with the high fidelity model.

System Setup:
\begin{itemize}
  \item High fidelity model/cost function: $J_h(u, x_0)$
  \item Low fidelity model/cost function: $J_l(u, x_0)$
  \item Control input in high fidelity model: $u_h$
  \item Size of $u_h$: $n_{u_h} \times 1$
  \item Control input in low fidelity model: $u_l$
  \item Size of $u_l$: $n_{u_l} \times 1$
  \item Random initial state: $x_0$
  \item High fidelity optimum: $u_h^*$
  \item Low fidelity optimum: $u_l^*$
\end{itemize}

$u_h$ and $u_l$ do not have the same dimensions, so to compare them we need to scale them to the same dimensions.

To upsample $u_l \rightarrow u_h$, we repeat the elements of $u_l$ to match the dimensions of $u_h$.
This can be seen as multiplying $u_l$ with a matrix $T_{lh}$ of size $n_{u_h} \times n_{u_l}$:

\begin{equation}
  T_{lh} =
  \begin{bmatrix}
    1 & 0 & \ldots & 0 \\
    1 & 0 & \ldots & 0 \\
    \vdots & \vdots & \ddots & \vdots \\
    0 & 1 & \ldots & 0 \\
    0 & 1 & \ldots & 0 \\
    \vdots & \vdots & \ddots & \vdots \\
    0 & 0 & \ldots & 1 \\
    0 & 0 & \ldots & 1 \\
    \vdots & \vdots & \ddots & \vdots \\
  \end{bmatrix}
\end{equation}

where the number of repeating ones in each column is determined by the ratio $r_{hl} = n_{u_h} / n_{u_l}$.
This gives us $u_{lh} = T_{lh} u_l$, which can be compared with $u_h$.

To downsample $u_h \rightarrow u_l$, we can either restrict the dimensions of $u_h$ to match $u_l$ or average the elements of $u_h$ to match the dimensions of $u_l$.

In case of restriction, we can multiply $u_h$ with a matrix $T_{hlr}$ of size $n_{u_l} \times n_{u_h}$:

\begin{equation}
  T_{hlr} =
  \begin{bmatrix}
    1 & 0 & \ldots & 0 & 0 \\
    0 & \ldots & 1 & \ldots & 0 \\
    \vdots & \ddots & \vdots & \ddots & \vdots \\
    0 & 0 & \ldots & 0 & 1 \\
  \end{bmatrix}
\end{equation}

Where the number of columns after which the next one is selected is determined by the ratio $r_{hl} = n_{u_l} / n_{u_h}$.
This gives us $u_{hlr} = T_{hlr} u_h$, which can be compared with $u_l$.

To downsample $u_h$ to $u_l$ by averaging, we need the ratio $r_{lh} = n_{u_h} \ n_{u_l}$.
We can then multiply $u_h$ with a matrix $T_{hla}$ of size $n_{u_l} \times n_{u_h}$:

\begin{equation}
  T_{hla} =
  \begin{bmatrix}
    r_{lh} & \ldots & 0 & 0 & 0 \\
    0 & \ldots & r_{lh} & \ldots & 0 \\
    \vdots & \ddots & \vdots & \vdots & \vdots \\
    0 & 0 & 0 & r_{lh} & \ldots \\
  \end{bmatrix}
\end{equation}

Where $r_{lh}$ is repeated $r_{hl}$ times in each row.
This gives us $u_{hla} = T_{hla} u_h$, which can be compared with $u_l$.

\subsection{Correlation between high and low fidelity models}

$u_h$ and $u_l$ each have $n_{u_h}$ and $n_{u_l}$ dimensions respectively, which represent the time steps of the simulation.
After converting them to the same dimensions as done in section \ref{comp_opt},
the cost for some realizations of the initial state $x_0$ can be computed.
Hence, a set of costs of both high and low fidelity control inputs can be computed for a certain model/cost function.
The correlation coefficient $\rho$ can then be computed between the distribution of the cost function values.
To illustrate this, assume there are $N$ realizations of the initial state $x_0$, and $J$ is the cost function.
$u_h$ and $u_l$ are the control inputs for the high and low fidelity models respectively, where the dimensions have been
converted to the same number as described in section \ref{comp_opt}. $\mu_h$ and $\mu_l$ are the mean costs
of the high and low fidelity control inputs respectively, and $\sigma_h$ and $\sigma_l$ are the standard deviations of
the costs of the high and low fidelity control inputs respectively.

\begin{equation}
  \rho(J(u_h), J(u_l)) = \frac{1}{N-1} \sum_{i=1}^{N} \left( \frac{J(u_h, x_0^i) - \mu_h}{\sigma_h} \right) \left( \frac{J(u_l, x_0^i) - \mu_l}{\sigma_l} \right)
\end{equation}

This can be simplified to:

\begin{equation}
  \rho(J(u_h), J(u_l)) = \frac{\text{Cov}(J(u_h), J(u_l))}{\sigma_h \sigma_l}
\end{equation}

This correlation coefficient $\rho$ can be calculated using the MATLAB function \href{https://www.mathworks.com/help/matlab/ref/corrcoef.html#bunkaln}{\texttt{corrcoef}}.

The correlation can also be calculated analytically using the covariance of the costs in the high fidelity objective function and the low fidelity objective function.
Let

\begin{itemize}
  \item $J_h$: high fidelity cost function
  \item $U_1$: high fidelity control input
  \item $U_2$: upsampled low fidelity control input $u_{lh} = T_{lh} u_l$
  \item $J_1 = J_h(U_1, x_0)$
  \item $\sigma_1$: standard deviation of $J_1$, $\sqrt{\text{Var}[J_1]}$
  \item $\sigma_2$: standard deviation of $J_2$, $\sqrt{\text{Var}[J_2]}$
\end{itemize}

Then correlation coefficient between $J_1$ and $J_2$ is

\begin{equation}
  \begin{aligned}
    \rho(J_1, J_2) &= \frac{\text{Cov}(J_1, J_2)}{\sigma_1 \sigma_2} \\
    &= \frac{\text{Cov}(J_1, J_2)}{\sqrt{\text{Var}[J_1]} \sqrt{\text{Var}[J_2]}}
  \end{aligned}
\end{equation}

Where $\text{Cov}(J_1, J_2)$, $\text{Var}[J_1]$ and $\text{Var}[J_2]$ can be calculated using the eqations in section \ref{lqr_ctrl}.

\subsection{Correlation Experiments}

We conducted experiments to check the correlation in 2 scenarios:
\begin{itemize}
  \item Perturbation in the direction of steepest ascent
  \item Along the path of a numerical optimizer
\end{itemize}

\subsubsection{Perturbation in the direction of steepest ascent}

Since the solution of the cost function is determined analytically, we cannot use it to determine the direction of steepest ascent.
The gradient at the solution will be 0. To get the direction of steepest ascent, the solution was perturbed slightly in random directions and the direction with the highest gradient
was chosen as the direction of steepest ascent.

\textbf{Perturbation in the high fidelity model/cost function}

When perturbing in the high fidelity model/cost function, the perturbed high fidelity solution was downsampled to low fidelity
dimensions by either restricting or averaging as described in section \ref{comp_opt}.

Hence, we have the following important variables:

\begin{itemize}
  \item High fidelity model/cost function: $J_h$
  \item Low fidelity model/cost function: $J_l$
  \item High fidelity optimum: $u_h^*$
  \item High fidelity perturbation: $p_h$
  \item Perturbed high fidelity solution: $u_h = u_h^* + p_h$
  \item Perturbed high fidelity solution downsampled using restriction: $u_{hlr} = T_{hlr} u_h$
  \item Perturbed high fidelity solution downsampled using averaging: $u_{hla} = T_{hla} u_h$
\end{itemize}

Comparing $J_h(u_h, x_0)$ and $J_l(u_{hlr}, x_0)$

Comparing $J_h(u_h, x_0)$ and $J_l(u_{hla}, x_0)$

\textbf{Perturbation in the low fidelity model/cost function}

When perturbing in the low fidelity model/cost function, the perturbed high fidelity solution was determined
by converting the low fidelity perturbation to high fidelity dimensions as described in section \ref{comp_opt}.

Hence, we have the following important variables:

\begin{itemize}
  \item High fidelity model/cost function: $J_h$
  \item Low fidelity model/cost function: $J_l$
  \item Low fidelity optimum: $u_l^*$
  \item Low fidelity perturbation: $p_l$
  \item Perturbed low fidelity solution: $u_l = u_l^* + p_l$
  \item Perturbed high fidelity solution upsampled: $u_{lh} = T_{lh} u_l$
\end{itemize}

Comparing $J_h(u_{lh}, x_0)$ and $J_l(u_l, x_0)$

\subsubsection{Along the path of a numerical optimizer}

Since in a realistic scenario, one would be optimizing using a numerical optimizer, and desire to reach the high fidelity optimum,
we conducted experiments to check the correlation along the path of a numerical optimizer in the high fidelity model/cost function.

Here, we warm start with the low fidelity optimum, and check the correlation by downsampling the high fidelity solution using averaging.

Hence, we have the following important variables:

\begin{itemize}
  \item High fidelity model/cost function: $J_h$
  \item Low fidelity model/cost function: $J_l$
  \item High fidelity solution: $u_h$
  \item High fidelity solution downsampled using restriction: $u_{hlr} = T_{hlr} u_h$
  \item High fidelity solution downsampled using averaging: $u_{hla} = T_{hla} u_h$
\end{itemize}

Comparing $J_h(u_h, x_0)$ and $J_h(u_{hlr}, x_0)$

Comparing $J_h(u_h, x_0)$ and $J_l(u_{hla}, x_0)$

\end{document}
